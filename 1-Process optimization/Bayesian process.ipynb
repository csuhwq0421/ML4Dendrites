{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Process\n",
    "### In this stage, we use bayesian optimization to find the best CVD hyperparameters for the ReSe2 dendritic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emukit\n",
    "import GPy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#c‚àíAl2O3 substrate is represented as 1, and MgO is represented as 0.\n",
    "df_rese2 = pd.read_excel('Dataset_bayesian.xlsx',sheet_name='initial')\n",
    "df_rese2.columns = ['number','T_Re', 'T_Se', 'c_Re',\n",
    "       'f_H2', 'Sub', 'Fractal']\n",
    "df_rese2.iloc[:,-1] = df_rese2.iloc[:,-1] *1\n",
    "\n",
    "print(df_rese2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the variable space and step size of the CVD experiment.\n",
    "T_Re_min, T_Re_max, T_Re_step = [580, 680, 10] ## 11 steps\n",
    "T_Re_var = np.arange(T_Re_min, T_Re_max+T_Re_step, T_Re_step)\n",
    "T_Re_num = len(T_Re_var)\n",
    "print(T_Re_var)\n",
    "print(T_Re_num)\n",
    "\n",
    "T_Se_min, T_Se_max, T_Se_step = [220, 300, 10] ## 9 steps\n",
    "T_Se_var = np.arange(T_Se_min, T_Se_max+T_Se_step, T_Se_step)\n",
    "T_Se_num = len(T_Se_var)\n",
    "print(T_Se_num)\n",
    "\n",
    "c_Re_min, c_Re_max, c_Re_step = [0.025, 0.15, 0.025] ## 6 steps\n",
    "c_Re_var = np.arange(c_Re_min, c_Re_max+c_Re_step, c_Re_step) \n",
    "c_Re_num = len(c_Re_var)\n",
    "print(c_Re_num)\n",
    "\n",
    "f_H2_min, f_H2_max, f_H2_step = [0.01, 0.04, 0.01] ## 4 steps\n",
    "f_H2_var = np.arange(f_H2_min, f_H2_max+f_H2_step, f_H2_step)\n",
    "f_H2_num = len(f_H2_var)\n",
    "print(f_H2_num)\n",
    "\n",
    "Sub_min,Sub_max, Sub_step = [0, 1, 1] ## 2 steps\n",
    "Sub_var = np.arange(Sub_min, Sub_max+Sub_step, Sub_step)\n",
    "Sub_num = len(Sub_var)\n",
    "print(Sub_num)\n",
    "\n",
    "\n",
    "\n",
    "var_array = [T_Re_var, T_Se_var, \n",
    "             c_Re_var, f_H2_var, \n",
    "             Sub_var]\n",
    "x_labels = ['T_Re', 'T_Se', 'c_Re',\n",
    "       'f_H2', 'Sub']\n",
    "\n",
    "def x_normalizer(X):\n",
    "    \n",
    "    def max_min_scaler(x, x_max, x_min):\n",
    "        return (x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    x_norm = []\n",
    "    for x in (X):\n",
    "           x_norm.append([max_min_scaler(x[i], \n",
    "                                         max(var_array[i]), \n",
    "                                         min(var_array[i])) for i in range(len(x))])  \n",
    "    return np.array(x_norm)\n",
    "\n",
    "def x_denormalizer(x_norm):\n",
    "    def max_min_rescaler(x, x_max, x_min):\n",
    "        return x*(x_max-x_min)+x_min\n",
    "    \n",
    "    x_original = []\n",
    "    for x in (x_norm):\n",
    "           x_original.append([max_min_rescaler(x[i], \n",
    "                                         max(var_array[i]), \n",
    "                                         min(var_array[i])) for i in range(len(x))])\n",
    "    return np.array(x_original)\n",
    "\n",
    "\n",
    "\n",
    "def get_closest_array(suggested_x):\n",
    "    \n",
    "    def get_closest_value(given_value, array_list):\n",
    "        absolute_difference_function = lambda list_value : abs(list_value - given_value)\n",
    "        closest_value = min(array_list, key=absolute_difference_function)\n",
    "        return closest_value\n",
    "    \n",
    "    var_list = var_array\n",
    "    modified_array = []\n",
    "    for x in suggested_x:\n",
    "        modified_array.append([get_closest_value(x[i], var_list[i]) for i in range(len(x))])\n",
    "    return np.array(modified_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter, DiscreteParameter\n",
    "\n",
    "## Set the range of the parameter space after normalization\n",
    "parameter_space = ParameterSpace([ContinuousParameter('T_Re', 0, 1),\n",
    "                                 ContinuousParameter('T_Se', 0, 1),\n",
    "                                 ContinuousParameter('c', 0, 1),\n",
    "                                 ContinuousParameter('H2', 0, 1),\n",
    "                                 DiscreteParameter('Sub', np.linspace(0,1,2)),\n",
    "                                 ])\n",
    "print(parameter_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from emukit.core.acquisition import Acquisition\n",
    "from emukit.core.interfaces import IModel, IDifferentiable\n",
    "from emukit.core.loop import FixedIntervalUpdater, OuterLoop, SequentialPointCalculator\n",
    "from emukit.core.loop.loop_state import create_loop_state\n",
    "from emukit.core.optimization import GradientAcquisitionOptimizer\n",
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement\n",
    "from emukit.core.acquisition import IntegratedHyperParameterAcquisition\n",
    "from emukit.bayesian_optimization.local_penalization_calculator import LocalPenalizationPointCalculator\n",
    "\n",
    "# create a loop for building Bayesian Optimization\n",
    "class ProbabilisticBayesianOptimizationLoop(OuterLoop):\n",
    "    def __init__(self, space: ParameterSpace, model_objective: Union[IModel, IDifferentiable],\n",
    "                 acquisition: Acquisition = None,\n",
    "                 update_interval: int = 1, batch_size: int = 1):\n",
    "\n",
    "        self.model_objective = model_objective\n",
    "        \n",
    "        if acquisition is None:\n",
    "            acquisition = ExpectedImprovement(model_objective)\n",
    "\n",
    "        model_updater_objective = FixedIntervalUpdater(model_objective, update_interval)\n",
    "\n",
    "        acquisition_optimizer = GradientAcquisitionOptimizer(space)\n",
    "        if batch_size == 1:\n",
    "            candidate_point_calculator = SequentialPointCalculator(acquisition, acquisition_optimizer)\n",
    "        else:\n",
    "            candidate_point_calculator = LocalPenalizationPointCalculator(acquisition, acquisition_optimizer,\n",
    "                                                                          model_objective, space, batch_size)\n",
    "        loop_state = create_loop_state(model_objective.X, model_objective.Y)\n",
    "\n",
    "        super(ProbabilisticBayesianOptimizationLoop, self).__init__(candidate_point_calculator,\n",
    "                                                                   [model_updater_objective],\n",
    "                                                                   loop_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GP Regression on the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(20)\n",
    "\n",
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "def y_normalizer(y_2d):\n",
    "     y_normalized=y_2d-1\n",
    "     return y_normalized\n",
    "\n",
    "def y_denormalizer(y_normalized):\n",
    "     y_original=y_normalized+1\n",
    "     return y_original\n",
    "x_init = x_normalizer(df_rese2.iloc[:,1:6].values)\n",
    "y_init = y_normalizer(np.transpose([df_rese2.iloc[:,-1].values]))\n",
    "X, Y = [x_init, y_init]\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "\n",
    "input_dim = len(X[0])\n",
    "print(input_dim)\n",
    "ker = GPy.kern.Matern32(input_dim = input_dim, ARD =True)#\n",
    "ker.lengthscale.constrain_bounded(1e-1, 10)\n",
    "ker.variance.constrain_bounded(1, 1000.0)\n",
    "\n",
    "#ker += GPy.kern.Bias(input_dim = input_dim)\n",
    "model_gpy = GPRegression(X , -Y, ker)#Emukit is a minimization tool; need to make Y negative\n",
    "model_gpy.Gaussian_noise.variance = 0.05**2\n",
    "model_gpy.Gaussian_noise.variance.fix()\n",
    "model_gpy.randomize()\n",
    "model_gpy.optimize_restarts(num_restarts=20,verbose =False, messages=False)\n",
    "objective_model = GPyModelWrapper(model_gpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the performance of the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_obj = objective_model.model.predict\n",
    "\n",
    "y_pred, y_uncer = f_obj(X)\n",
    "y_pred = -y_pred[:, -1]\n",
    "y_uncer = np.sqrt(y_uncer[:, -1])\n",
    "y_pred_unscaled = y_denormalizer(y_pred)\n",
    "\n",
    "print(y_pred_unscaled)\n",
    "print(y_uncer)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.5))  \n",
    "fs = 18\n",
    "lims1 = (1, 2)\n",
    "\n",
    "\n",
    "ax.scatter(y_denormalizer(Y[:, -1]), y_pred_unscaled, alpha=0.5, \n",
    "           c=(112/255, 161/255, 255/255), edgecolor='navy')\n",
    "ax.errorbar(y_denormalizer(Y[:, -1]), y_pred_unscaled, yerr=y_uncer, \n",
    "            ms=0, ls='', capsize=2, alpha=0.6, color='gray', zorder=0)\n",
    "ax.plot(lims1, lims1, 'k--', alpha=0.75, zorder=0)\n",
    "\n",
    "rmse_value = np.sqrt(mean_squared_error(Y[:, -1], y_pred))\n",
    "\n",
    "ax.set_xlabel('True Values', fontsize=fs)\n",
    "ax.set_ylabel('Predicted Values', fontsize=fs)\n",
    "ax.tick_params(labelsize=fs-2)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new parameters for which predictions are needed\n",
    "new_params = np.array([[580, 220, 0.075, 0.02, 1]])\n",
    "\n",
    "# Use the model to predict new parameters, obtaining both the prediction values and uncertainty estimates\n",
    "new_pred, new_uncer = objective_model.model.predict(x_normalizer(new_params))\n",
    "\n",
    "new_pred = -new_pred[:, -1]\n",
    "new_pred_unscaled = y_denormalizer(new_pred)\n",
    "\n",
    "new_uncer = np.sqrt(new_uncer[:, -1])\n",
    "\n",
    "print(\"Prediction (after denormalization):\", new_pred_unscaled)\n",
    "print(\"Uncertainty Estimate:\", new_uncer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "mse = mean_squared_error\n",
    "mse_all = mse(Y[:,-1], y_pred)\n",
    "print ('all RMSE: %.4f' % (np.sqrt(mse_all)))\n",
    "\n",
    "rsquared_all = r2_score(Y[:,-1], y_pred)\n",
    "print ('all R^2: %.4f' % (rsquared_all))\n",
    "\n",
    "sprman_all = spearmanr(Y[:,-1], y_pred)\n",
    "print ('all spearman: %.4f' % (sprman_all[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the First Run of the Batch-mode Bayesian Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement, \\\n",
    "                                                      NegativeLowerConfidenceBound, \\\n",
    "                                                      MaxValueEntropySearch, \\\n",
    "                                                      MultipointExpectedImprovement,\\\n",
    "                                                      ProbabilityOfFeasibility, \\\n",
    "                                                      ProbabilityOfImprovement\n",
    "# Expeceted Improvement (EI)\n",
    "# acquisition = ExpectedImprovement(objective_model, jitter=0.05)\n",
    "# acquisition_generator = lambda m: ExpectedImprovement(m, jitter=0.2)\n",
    "# acquisition_integrated = IntegratedHyperParameterAcquisition(objective_model, acquisition_generator)\n",
    "# acquisition = acquisition_integrated\n",
    "\n",
    "## Lower Confidence Bound (LCB)\n",
    "# acquisition = NegativeLowerConfidenceBound(objective_model, beta = 10)\n",
    "## Probability Improvement (PI)\n",
    "# acquisition = ProbabilityOfImprovement(objective_model,jitter=0)\n",
    "\n",
    "\n",
    "\n",
    "# MaxValueEntropySearch (MES)\n",
    "acquisition_generator = lambda m: MaxValueEntropySearch(m, parameter_space)\n",
    "acquisition_integrated = IntegratedHyperParameterAcquisition(objective_model, acquisition_generator)\n",
    "acquisition = acquisition_integrated\n",
    "\n",
    "# Create a Bayesian optimization loop and collect new sample points\n",
    "bayesopt_cons_pr = ProbabilisticBayesianOptimizationLoop(\n",
    "    model_objective=objective_model,\n",
    "    space=parameter_space,\n",
    "    acquisition=acquisition,\n",
    "    batch_size=15  # batchsize>10 to account for duplication\n",
    ")\n",
    "\n",
    "X_new = bayesopt_cons_pr.candidate_point_calculator.compute_next_points(bayesopt_cons_pr.loop_state)\n",
    "X_new1= x_denormalizer(X_new)\n",
    "\n",
    "f_obj = objective_model.model.predict\n",
    "\n",
    "y_pred_new, y_uncer_new = f_obj(X_new)\n",
    "y_pred_new = -y_pred_new\n",
    "y_uncer_new = np.sqrt(y_uncer_new)\n",
    "print(y_pred_new)\n",
    "\n",
    "\n",
    "df_Xnew = pd.DataFrame(get_closest_array(X_new1), columns=df_rese2.columns[1:6])\n",
    "df_all = pd.concat([df_rese2.iloc[:, 1:6], df_Xnew])\n",
    "df_all_ = df_all.drop_duplicates()\n",
    "df_Xnew = df_Xnew.sort_values(by=list(df_rese2.columns[1:6]), ignore_index = True)\n",
    "df_Xnew.index = np.arange(len(df_Xnew))+len(df_rese2)\n",
    "df_Xnew.iloc[:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xnew_array = df_Xnew.iloc[:, :].values\n",
    "result_array = get_closest_array(df_Xnew_array)\n",
    "\n",
    "print(result_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Surragate Model\n",
    "y_pred_new, y_uncer_new = f_obj(x_normalizer(result_array))\n",
    "y_pred_new = -y_pred_new\n",
    "y_uncer_new = np.sqrt(y_uncer_new)\n",
    "# Aquisition function\n",
    "f_acq = bayesopt_cons_pr.candidate_point_calculator.acquisition.evaluate \n",
    "acq_produc = f_acq(x_normalizer(result_array)) \n",
    "print(y_denormalizer(y_pred_new))\n",
    "print(acq_produc)\n",
    "\n",
    "\n",
    "X_new_columns = [f'X_new_{i}' for i in range(df_Xnew.shape[1])]\n",
    "X_new_data = {col: result_array[:, i] for i, col in enumerate(X_new_columns)}\n",
    "df_X_new = pd.DataFrame(X_new_data)\n",
    "df_X_new['y_pred_new'] =y_denormalizer(y_pred_new) \n",
    "df_X_new['y_uncer_new'] = y_uncer_new\n",
    "df_X_new['acq_produc'] = acq_produc\n",
    "\n",
    "\n",
    "output_filename = \"Next_round_suggestion.xlsx\"\n",
    "df_X_new.to_excel(output_filename, index=False)\n",
    "print(f\"The data has been successfully saved to an Excel file {output_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Dataset_bayesian.xlsx'  # ‰øÆÊîπ‰∏∫‰Ω†ÁöÑExcelÊñá‰ª∂Ë∑ØÂæÑ\n",
    "df_sug = pd.read_excel(file_path, sheet_name='sug-r3')\n",
    "\n",
    "\n",
    "X_new = x_normalizer(df_sug.iloc[:, :5].values)\n",
    "\n",
    "# ÊâìÂç∞ÁªìÊûúÈ™åËØÅ\n",
    "print(\"X_newÂèòÈáèÂÜÖÂÆπÔºàÂâç5Ë°åÔºâÔºö\")\n",
    "# Âõ†‰∏∫X_newÊòØÊï∞ÁªÑÔºåÊàë‰ª¨ÊâìÂç∞Ââç5Ë°å\n",
    "print(X_new[:5])\n",
    "\n",
    "# Ëé∑ÂèñÂàóÂêçÔºà‰ªéÂéüÂßãDataFrame‰∏≠Ëé∑ÂèñÂâç‰∫îÂàóÁöÑÂàóÂêçÔºâ\n",
    "column_names = list(df_sug.columns[:5])\n",
    "print(\"\\nÂâç‰∫îÂàóÂàóÂêçÔºö\", column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the data of the 3D CVD parameter-DF plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emukit.core.initial_designs.random_design import RandomDesign\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import pandas as pd\n",
    "\n",
    "cmap_colors = [\n",
    "    (30/255, 136/255, 229/255),   # Blue\n",
    "    (1.0, 1.0, 1.0),             # White\n",
    "    (255/255, 13/255, 87/255)    # Pink \n",
    "]\n",
    "\n",
    "# Create color map\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"my_colormap\", cmap_colors)\n",
    "\n",
    "np.random.seed(28)\n",
    "design = RandomDesign(parameter_space)\n",
    "x_sampled = design.get_samples(600)\n",
    "x_columns = df_rese2.iloc[:,1:6].columns\n",
    "x_sampled_df = pd.DataFrame(x_denormalizer(x_sampled), columns=x_columns)\n",
    "\n",
    "def x_normalizer_for_z(z_values, z_min, z_max):\n",
    "    \"\"\"\n",
    "    Normalize given z values.\n",
    "    \n",
    "    Parameters:\n",
    "    - z_values: The original z value or array of z values to normalize.\n",
    "    - z_min: Normalization parameter specifying the minimum z value.\n",
    "    - z_max: Normalization parameter specifying the maximum z value.\n",
    "    \n",
    "    Returns:\n",
    "    - Normalized z value or array of z values.\n",
    "    \"\"\"\n",
    "    return (np.array(z_values) - z_min) / (z_max - z_min)\n",
    "\n",
    "# Assume that the z-axis corresponds to T_Re_var, i.e., the range of z-axis values is [T_Re_min, T_Re_max]\n",
    "\n",
    "\n",
    "def create_3d_stacked_heatmaps(dim1, dim2, dim3, metric_type='mean', uncertainty=False, z_values=[0.05,0.1,0.15], n_grid=21):\n",
    "    print(\"\\n[Info] Generating 3D stacked heatmap data...\")\n",
    "    # Use provided z values instead of uniform distribution\n",
    "    z_levels = z_values\n",
    "    heatmap_layers = []\n",
    "    z_positions_original = []\n",
    "\n",
    "    for z_idx, z_val in enumerate(z_levels):\n",
    "        print(f\"  --> Processing layer {z_idx+1}/{len(z_levels)}... \")\n",
    "        x1x2y_data = []\n",
    "\n",
    "        # Normalize z value\n",
    "        z_normalized = x_normalizer_for_z(np.array([z_val]), T_Re_min, T_Re_max)[0]\n",
    "\n",
    "        for x1 in np.linspace(0, 1, n_grid):\n",
    "            for x2 in np.linspace(0, 1, n_grid):\n",
    "                x_base = np.mean(x_sampled, axis=0)\n",
    "                x_base[dim1] = x1\n",
    "                x_base[dim2] = x2\n",
    "                x_base[dim3] = z_normalized # Use normalized z value\n",
    "                x_input = x_base.reshape(1, -1)\n",
    "\n",
    "                y_pred, y_uncer = f_obj(x_input)\n",
    "                y_pred = -y_pred + 1\n",
    "\n",
    "                if uncertainty:\n",
    "                    uncer_val = np.sqrt(np.abs(y_uncer[0]))  # Avoid sqrt(negative) causing NaN\n",
    "                    val = uncer_val if not np.isnan(uncer_val) else 0.0\n",
    "                else:\n",
    "                    if metric_type == 'max':\n",
    "                        val = np.max(y_pred)\n",
    "                    elif metric_type == 'min':\n",
    "                        val = np.min(y_pred)\n",
    "                    else:\n",
    "                        val = np.mean(y_pred)\n",
    "\n",
    "                x_org = x_denormalizer(x_input)[0]\n",
    "                x1x2y_data.append([x_org[dim1], x_org[dim2], val])\n",
    "\n",
    "        data_array = np.array(x1x2y_data, dtype=float)\n",
    "        x1_grid = data_array[:, 0].reshape(n_grid, n_grid)\n",
    "        x2_grid = data_array[:, 1].reshape(n_grid, n_grid)\n",
    "        y_grid = data_array[:, 2].reshape(n_grid, n_grid)\n",
    "        heatmap_layers.append((x1_grid, x2_grid, y_grid))\n",
    "\n",
    "        x_for_z = np.mean(x_sampled, axis=0)\n",
    "        x_for_z[dim3] = z_val\n",
    "        \n",
    "        z_positions_original.append(z_val)\n",
    "    \n",
    "    # 3D plot\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Color bar\n",
    "    vmin, vmax = (0.05, 0.10) if uncertainty else (1.1, 1.8)\n",
    "    title_prefix = \"Uncertainty\" if uncertainty else \"Objective Function\"\n",
    "\n",
    "    for i, (x1_grid, x2_grid, y_grid) in enumerate(heatmap_layers):\n",
    "        z_grid = np.full_like(x1_grid, z_positions_original[i])\n",
    "        normalized = (y_grid - vmin) / (vmax - vmin)\n",
    "        normalized = np.clip(normalized, 0, 1).astype(float)\n",
    "        facecolors = cmap(normalized)\n",
    "\n",
    "        # Heatmap settings\n",
    "        ax.plot_surface(\n",
    "            x1_grid, x2_grid, z_grid,\n",
    "            facecolors=facecolors,\n",
    "            alpha=1.0, linewidth=0.0, antialiased=True, shade=False\n",
    "        )\n",
    "        ax.contour(x1_grid, x2_grid, z_grid, levels=[z_positions_original[i]],\n",
    "                   colors='black', alpha=0.8, linewidths=2.0)\n",
    "\n",
    "    X_orig = x_denormalizer(X)\n",
    "    ax.scatter(X_orig[:, dim1], X_orig[:, dim2], X_orig[:, dim3],\n",
    "               c='gray', s=250, alpha=0.9, edgecolors='black', linewidth=1, label='Original samples')\n",
    "\n",
    "    X_new_orig = x_denormalizer(X_new)\n",
    "    ax.scatter(X_new_orig[:, dim1], X_new_orig[:, dim2], X_new_orig[:, dim3],\n",
    "               c='black', s=300, alpha=1.0, edgecolors='darkgreen', linewidth=2,\n",
    "               marker='*', label='New samples')\n",
    "\n",
    "    ax.set_xlabel(f'{x_columns[dim1]}', fontsize=14)\n",
    "    ax.set_ylabel(f'{x_columns[dim2]}', fontsize=14)\n",
    "    ax.set_zlabel(f'{x_columns[dim3]}', fontsize=14)\n",
    "    ax.set_title(f'3D Stacked Heatmaps: {title_prefix} ({metric_type})', fontsize=16, pad=20)\n",
    "\n",
    "    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, shrink=0.6, aspect=20, pad=0.1)\n",
    "    cbar.set_label(title_prefix, fontsize=12)\n",
    "    cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "    ax.legend(loc='upper right', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.view_init(elev=10, azim=30)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Create a Pandas Excel writer using xlsxwriter as the engine\n",
    "    output_excel = f\"heatmap_data_dim_{dim1}_{dim2}_{dim3}.xlsx\"\n",
    "    \n",
    "    with pd.ExcelWriter(output_excel, engine='xlsxwriter') as writer:\n",
    "        for i, (x1_grid, x2_grid, y_grid) in enumerate(heatmap_layers):\n",
    "            z_val = z_positions_original[i]\n",
    "            print(f\"--> Saving data for z={z_val} to Excel...\")\n",
    "            \n",
    "            # Flatten grid data\n",
    "            data_flat = np.column_stack((\n",
    "                x1_grid.ravel(), \n",
    "                x2_grid.ravel(), \n",
    "                y_grid.ravel()\n",
    "            ))\n",
    "            \n",
    "            df = pd.DataFrame(data_flat, columns=[f'Dim{dim1}', f'Dim{dim2}', 'Value'])\n",
    "            \n",
    "            # Write to different sheets in Excel\n",
    "            sheet_name = f'z_{int(z_val)}'\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Data successfully saved to {output_excel}\")\n",
    "\n",
    "    return fig, ax, heatmap_layers, z_positions_original\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Select three feature dimensions for visualization\n",
    "    feature_dims = [1, 2, 0]  # Adjust according to your needs\n",
    "    \n",
    "    # print(\"Creating 3D stacked heatmap visualization...\")\n",
    "    fig1, ax1, layers1, z_pos1 = create_3d_stacked_heatmaps(\n",
    "        feature_dims[0], feature_dims[1], feature_dims[2], \n",
    "        metric_type='max', uncertainty=False, z_values=[580, 620, 660], n_grid=21\n",
    "    )\n",
    "\n",
    "    # Also show uncertainty\n",
    "    print(\"Creating 3D stacked heatmap of uncertainty...\")\n",
    "    fig3, ax3, layers3, z_pos3 = create_3d_stacked_heatmaps(\n",
    "        feature_dims[0], feature_dims[1], feature_dims[2], \n",
    "        metric_type='max', uncertainty=True, z_values=[580, 620, 660], n_grid=21\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the data of the 2D CVD parameter-DF plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from emukit.core.initial_designs.random_design import RandomDesign\n",
    "import os\n",
    "\n",
    "# Set the output path\n",
    "output_folder = \"C:\\\\Users\\\\ÈªÑÊñáÂº∫\\\\Desktop\\\\project\\\\ML4dentrites\\\\1-Process optimization\\\\process_data_r1\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "input_dim = 5  # Number of input dimensions, e.g., 5 features\n",
    "\n",
    "# Create color maps\n",
    "mean_cmap_colors = [\n",
    "    (30/255, 136/255, 229/255),   # Blue\n",
    "    (1.0, 1.0, 1.0),             # White\n",
    "    (255/255, 13/255, 87/255)    # Pink \n",
    "]\n",
    "mean_cmap = mcolors.LinearSegmentedColormap.from_list(\"mean_colormap\", mean_cmap_colors, N=256)\n",
    "\n",
    "# Three-color gradient for uncertainty heatmaps: dark blue ‚Üí white ‚Üí red\n",
    "uncertainty_cmap_colors = [\n",
    "    (0/255, 0/255, 255/255),     # Dark blue\n",
    "    (1.0, 1.0, 1.0),             # White\n",
    "    (255/255, 0/255, 0/255)      # Red\n",
    "]\n",
    "uncertainty_cmap = mcolors.LinearSegmentedColormap.from_list(\"uncertainty_colormap\", uncertainty_cmap_colors, N=256)\n",
    "\n",
    "mean_levels = np.linspace(1.1,1.8,42)\n",
    "uncertainty_levels = np.linspace(0.05, 0.15, 60)\n",
    "\n",
    "# Set random seed and generate samples\n",
    "np.random.seed(10)\n",
    "design = RandomDesign(parameter_space)\n",
    "x_sampled = design.get_samples(200)\n",
    "x_columns = df_rese2.iloc[:,1:6].columns\n",
    "x_sampled_df = pd.DataFrame(x_denormalizer(x_sampled), columns=x_columns)\n",
    "\n",
    "for i in range(input_dim):\n",
    "    for j in range(input_dim - i - 1):\n",
    "        ind1 = i\n",
    "        ind2 = j + i + 1\n",
    "        \n",
    "        n_steps = 21\n",
    "        x1x2y_pred = []\n",
    "        x1x2y_uncer = []\n",
    "\n",
    "        # Build grid and calculate predictions and uncertainties\n",
    "        for x1 in np.linspace(0, 1, n_steps):\n",
    "            for x2 in np.linspace(0, 1, n_steps):\n",
    "                x_temp = np.copy(x_sampled)\n",
    "                x_temp[:, ind1] = x1\n",
    "                x_temp[:, ind2] = x2\n",
    "                y_pred, y_uncer = f_obj(x_temp)\n",
    "                y_pred = -y_pred + 1  # Optional: Adjust prediction values direction\n",
    "                \n",
    "                x1_org = x_denormalizer(x_temp)[0, ind1]\n",
    "                x2_org = x_denormalizer(x_temp)[0, ind2]\n",
    "\n",
    "                # Collect prediction statistics\n",
    "                x1x2y_pred.append([\n",
    "                    x1_org, x2_org,\n",
    "                    np.max(y_pred),\n",
    "                    np.mean(y_pred),\n",
    "                    np.min(y_pred)\n",
    "                ])\n",
    "\n",
    "                # Collect uncertainty statistics\n",
    "                x1x2y_uncer.append([\n",
    "                    x1_org, x2_org,\n",
    "                    np.max(np.sqrt(y_uncer)),\n",
    "                    np.mean(np.sqrt(y_uncer)),\n",
    "                    np.min(np.sqrt(y_uncer))\n",
    "                ])\n",
    "        \n",
    "        # Convert to numpy arrays and reshape\n",
    "        x1 = np.array([row[0] for row in x1x2y_pred], dtype=float).reshape(n_steps, n_steps)\n",
    "        x2 = np.array([row[1] for row in x1x2y_pred], dtype=float).reshape(n_steps, n_steps)\n",
    "\n",
    "        y_pred_max = np.array([row[2] for row in x1x2y_pred], dtype=float).reshape(n_steps, n_steps)\n",
    "        y_pred_mean = np.array([row[3] for row in x1x2y_pred], dtype=float).reshape(n_steps, n_steps)\n",
    "        y_pred_min = np.array([row[4] for row in x1x2y_pred], dtype=float).reshape(n_steps, n_steps)\n",
    "\n",
    "        y_uncer_max = np.array([row[2] for row in x1x2y_uncer], dtype=float).reshape(n_steps, n_steps)\n",
    "        y_uncer_mean = np.array([row[3] for row in x1x2y_uncer], dtype=float).reshape(n_steps, n_steps)\n",
    "        y_uncer_min = np.array([row[4] for row in x1x2y_uncer], dtype=float).reshape(n_steps, n_steps)\n",
    "\n",
    "        fs = 20\n",
    "        title_pad = 16\n",
    "\n",
    "        # Plot Prediction figures\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(17, 3.5), sharey=False, sharex=False)\n",
    "        for ax, y, title in zip(axes, [y_pred_max, y_pred_mean, y_pred_min],\n",
    "                                ['Max Prediction', 'Mean Prediction', 'Min Prediction']):\n",
    "            c_plt1 = ax.contourf(x1, x2, y, levels=mean_levels, cmap=mean_cmap, extend='both')\n",
    "            cbar = fig.colorbar(c_plt1, ax=ax)\n",
    "            cbar.ax.tick_params(labelsize=fs*0.8)\n",
    "\n",
    "            ax.scatter(x_denormalizer(X)[:, ind1], x_denormalizer(X)[:, ind2],\n",
    "                       s=80, facecolors='gray', alpha=0.9, edgecolor='gray')\n",
    "            ax.scatter(x_denormalizer(X_new)[:, ind1], x_denormalizer(X_new)[:, ind2],\n",
    "                       s=100, facecolors='orange', alpha=0.9, edgecolor='orange')\n",
    "\n",
    "            ax.set_xlabel(str(x_columns[ind1]), fontsize=fs)\n",
    "            ax.set_ylabel(str(x_columns[ind2]), fontsize=fs)\n",
    "\n",
    "            x1_delta = (np.max(x1)-np.min(x1)) * 0.04\n",
    "            x2_delta = (np.max(x2)-np.min(x2)) * 0.04\n",
    "            ax.set_xlim(np.min(x1)-x1_delta, np.max(x1)+x1_delta)\n",
    "            ax.set_ylim(np.min(x2)-x2_delta, np.max(x2)+x2_delta)\n",
    "            ax.tick_params(direction='in', length=5, width=1, labelsize=fs*0.8)\n",
    "\n",
    "            if ind1 == 0:\n",
    "                ax.set_xticks([580, 610, 640, 670])\n",
    "            if ind1 == 1:\n",
    "                ax.set_xticks([220, 240, 260, 280, 300])\n",
    "            if ind1 == 2:\n",
    "                ax.set_xticks([0.05, 0.1, 0.15])\n",
    "            if ind2 == 4:\n",
    "                ax.set_yticks([0, 1])\n",
    "\n",
    "        axes[0].set_title('Objective function max', pad=title_pad, fontsize=fs)\n",
    "        axes[1].set_title('Objective function mean', pad=title_pad, fontsize=fs)\n",
    "        axes[2].set_title('Objective function min', pad=title_pad, fontsize=fs)\n",
    "        plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "        # Save Prediction figures\n",
    "        feature1 = x_columns[ind1]\n",
    "        feature2 = x_columns[ind2]\n",
    "        pred_image_path = os.path.join(output_folder, f\"{feature1}_vs_{feature2}_prediction.png\")\n",
    "        plt.savefig(pred_image_path, dpi=600, bbox_inches='tight')  # High resolution, tight margins\n",
    "        plt.close(fig)  # Close current figure to save memory\n",
    "\n",
    "        # Plot Uncertainty figures\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(17, 3.5), sharey=False, sharex=False)\n",
    "        for ax, y, title in zip(axes, [y_uncer_max, y_uncer_mean, y_uncer_min],\n",
    "                                ['Max Uncertainty', 'Mean Uncertainty', 'Min Uncertainty']):\n",
    "            c_plt1 = ax.contourf(x1, x2, y, levels=uncertainty_levels, cmap=uncertainty_cmap, extend='both')\n",
    "            cbar = fig.colorbar(c_plt1, ax=ax)\n",
    "            cbar.ax.tick_params(labelsize=fs*0.8)\n",
    "\n",
    "            ax.scatter(x_denormalizer(X)[:, ind1], x_denormalizer(X)[:, ind2],\n",
    "                       s=80, facecolors='gray', alpha=0.9, edgecolor='gray')\n",
    "            ax.scatter(x_denormalizer(X_new)[:, ind1], x_denormalizer(X_new)[:, ind2],\n",
    "                       s=100, facecolors='orange', alpha=0.9, edgecolor='orange')\n",
    "\n",
    "            ax.set_xlabel(str(x_columns[ind1]), fontsize=fs)\n",
    "            ax.set_ylabel(str(x_columns[ind2]), fontsize=fs)\n",
    "\n",
    "            x1_delta = (np.max(x1)-np.min(x1)) * 0.04\n",
    "            x2_delta = (np.max(x2)-np.min(x2)) * 0.04\n",
    "            ax.set_xlim(np.min(x1)-x1_delta, np.max(x1)+x1_delta)\n",
    "            ax.set_ylim(np.min(x2)-x2_delta, np.max(x2)+x2_delta)\n",
    "            ax.tick_params(direction='in', length=5, width=1, labelsize=fs*0.8)\n",
    "\n",
    "            if ind1 == 0:\n",
    "                ax.set_xticks([580, 610, 640, 670])\n",
    "            if ind1 == 1:\n",
    "                ax.set_xticks([220, 240, 260, 280, 300])\n",
    "            if ind1 == 2:\n",
    "                ax.set_xticks([0.05, 0.1, 0.15])\n",
    "            if ind2 == 4:\n",
    "                ax.set_yticks([0, 1])\n",
    "\n",
    "        axes[0].set_title('Objective uncertainty max', pad=title_pad, fontsize=fs)\n",
    "        axes[1].set_title('Objective uncertainty mean', pad=title_pad, fontsize=fs)\n",
    "        axes[2].set_title('Objective uncertainty min', pad=title_pad, fontsize=fs)\n",
    "        plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "        # Save Uncertainty figures\n",
    "        uncer_image_path = os.path.join(output_folder, f\"{feature1}_vs_{feature2}_uncertainty.png\")\n",
    "        plt.savefig(uncer_image_path, dpi=600, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Construct DataFrame and save as Excel\n",
    "        pred_df = pd.DataFrame(x1x2y_pred, columns=[feature1, feature2, 'Max Prediction', 'Mean Prediction', 'Min Prediction'])\n",
    "        uncer_df = pd.DataFrame(x1x2y_uncer, columns=[feature1, feature2, 'Max Uncertainty', 'Mean Uncertainty', 'Min Uncertainty'])\n",
    "\n",
    "        with pd.ExcelWriter(os.path.join(output_folder, f\"{feature1}_vs_{feature2}.xlsx\")) as writer:\n",
    "            pred_df.to_excel(writer, sheet_name='Prediction', index=False)\n",
    "            uncer_df.to_excel(writer, sheet_name='Uncertainty', index=False)\n",
    "\n",
    "        print(f\"üìä Saved {feature1}_vs_{feature2}.xlsx to {output_folder}\")\n",
    "        print(f\"üñºÔ∏è Saved images to: {pred_image_path} and {uncer_image_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
