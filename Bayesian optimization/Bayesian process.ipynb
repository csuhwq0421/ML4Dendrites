{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization Process\n",
    "### In this stage, we use bayesian optimization to find the best CVD hyperparameters for the ReSe2 dendritic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emukit\n",
    "import GPy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#câˆ’Al2O3 substrate is represented as 1, and MgO is represented as 0.\n",
    "df_rese2 = pd.read_excel('data_new.xlsx',sheet_name='initial')\n",
    "df_rese2.columns = ['number','T_Re', 'T_Se', 'c_Re',\n",
    "       'f_H2', 'Sub', 'Fractal']\n",
    "df_rese2.iloc[:,-1] = df_rese2.iloc[:,-1] *1\n",
    "\n",
    "print(df_rese2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the variable space and step size of the CVD experiment.\n",
    "T_Re_min, T_Re_max, T_Re_step = [580, 680, 10] ## 11 steps\n",
    "T_Re_var = np.arange(T_Re_min, T_Re_max+T_Re_step, T_Re_step)\n",
    "T_Re_num = len(T_Re_var)\n",
    "print(T_Re_var)\n",
    "print(T_Re_num)\n",
    "\n",
    "T_Se_min, T_Se_max, T_Se_step = [220, 300, 10] ## 9 steps\n",
    "T_Se_var = np.arange(T_Se_min, T_Se_max+T_Se_step, T_Se_step)\n",
    "T_Se_num = len(T_Se_var)\n",
    "print(T_Se_num)\n",
    "\n",
    "c_Re_min, c_Re_max, c_Re_step = [0.025, 0.15, 0.025] ## 6 steps\n",
    "c_Re_var = np.arange(c_Re_min, c_Re_max+c_Re_step, c_Re_step) \n",
    "c_Re_num = len(c_Re_var)\n",
    "print(c_Re_num)\n",
    "\n",
    "f_H2_min, f_H2_max, f_H2_step = [0.01, 0.04, 0.01] ## 4 steps\n",
    "f_H2_var = np.arange(f_H2_min, f_H2_max+f_H2_step, f_H2_step)\n",
    "f_H2_num = len(f_H2_var)\n",
    "print(f_H2_num)\n",
    "\n",
    "Sub_min,Sub_max, Sub_step = [0, 1, 1] ## 2 steps\n",
    "Sub_var = np.arange(Sub_min, Sub_max+Sub_step, Sub_step)\n",
    "Sub_num = len(Sub_var)\n",
    "print(Sub_num)\n",
    "\n",
    "\n",
    "\n",
    "var_array = [T_Re_var, T_Se_var, \n",
    "             c_Re_var, f_H2_var, \n",
    "             Sub_var]\n",
    "x_labels = ['T_Re', 'T_Se', 'c_Re',\n",
    "       'f_H2', 'Sub']\n",
    "\n",
    "def x_normalizer(X):\n",
    "    \n",
    "    def max_min_scaler(x, x_max, x_min):\n",
    "        return (x-x_min)/(x_max-x_min)\n",
    "    \n",
    "    x_norm = []\n",
    "    for x in (X):\n",
    "           x_norm.append([max_min_scaler(x[i], \n",
    "                                         max(var_array[i]), \n",
    "                                         min(var_array[i])) for i in range(len(x))])  \n",
    "    return np.array(x_norm)\n",
    "\n",
    "def x_denormalizer(x_norm):\n",
    "    def max_min_rescaler(x, x_max, x_min):\n",
    "        return x*(x_max-x_min)+x_min\n",
    "    \n",
    "    x_original = []\n",
    "    for x in (x_norm):\n",
    "           x_original.append([max_min_rescaler(x[i], \n",
    "                                         max(var_array[i]), \n",
    "                                         min(var_array[i])) for i in range(len(x))])\n",
    "    return np.array(x_original)\n",
    "\n",
    "\n",
    "\n",
    "def get_closest_array(suggested_x):\n",
    "    \n",
    "    def get_closest_value(given_value, array_list):\n",
    "        absolute_difference_function = lambda list_value : abs(list_value - given_value)\n",
    "        closest_value = min(array_list, key=absolute_difference_function)\n",
    "        return closest_value\n",
    "    \n",
    "    var_list = var_array\n",
    "    modified_array = []\n",
    "    for x in suggested_x:\n",
    "        modified_array.append([get_closest_value(x[i], var_list[i]) for i in range(len(x))])\n",
    "    return np.array(modified_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core import ParameterSpace, ContinuousParameter, DiscreteParameter\n",
    "\n",
    "## Set the range of the parameter space after normalization\n",
    "parameter_space = ParameterSpace([ContinuousParameter('T_Re', 0, 1),\n",
    "                                 ContinuousParameter('T_Se', 0, 1),\n",
    "                                 ContinuousParameter('c', 0, 1),\n",
    "                                 ContinuousParameter('H2', 0, 1),\n",
    "                                 DiscreteParameter('Sub', np.linspace(0,1,2)),\n",
    "                                 ])\n",
    "print(parameter_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from emukit.core.acquisition import Acquisition\n",
    "from emukit.core.interfaces import IModel, IDifferentiable\n",
    "from emukit.core.loop import FixedIntervalUpdater, OuterLoop, SequentialPointCalculator\n",
    "from emukit.core.loop.loop_state import create_loop_state\n",
    "from emukit.core.optimization import GradientAcquisitionOptimizer\n",
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement\n",
    "from emukit.core.acquisition import IntegratedHyperParameterAcquisition\n",
    "from emukit.bayesian_optimization.local_penalization_calculator import LocalPenalizationPointCalculator\n",
    "\n",
    "# create a loop for building Bayesian Optimization\n",
    "class ProbabilisticBayesianOptimizationLoop(OuterLoop):\n",
    "    def __init__(self, space: ParameterSpace, model_objective: Union[IModel, IDifferentiable],\n",
    "                 acquisition: Acquisition = None,\n",
    "                 update_interval: int = 1, batch_size: int = 1):\n",
    "\n",
    "        self.model_objective = model_objective\n",
    "        \n",
    "        if acquisition is None:\n",
    "            acquisition = ExpectedImprovement(model_objective)\n",
    "\n",
    "        model_updater_objective = FixedIntervalUpdater(model_objective, update_interval)\n",
    "\n",
    "        acquisition_optimizer = GradientAcquisitionOptimizer(space)\n",
    "        if batch_size == 1:\n",
    "            candidate_point_calculator = SequentialPointCalculator(acquisition, acquisition_optimizer)\n",
    "        else:\n",
    "            candidate_point_calculator = LocalPenalizationPointCalculator(acquisition, acquisition_optimizer,\n",
    "                                                                          model_objective, space, batch_size)\n",
    "        loop_state = create_loop_state(model_objective.X, model_objective.Y)\n",
    "\n",
    "        super(ProbabilisticBayesianOptimizationLoop, self).__init__(candidate_point_calculator,\n",
    "                                                                   [model_updater_objective],\n",
    "                                                                   loop_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GP Regression on the collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed=20\n",
    "\n",
    "from GPy.models import GPRegression\n",
    "from emukit.model_wrappers import GPyModelWrapper\n",
    "def y_normalizer(y_2d):\n",
    "     y_normalized=y_2d-1\n",
    "     return y_normalized\n",
    "\n",
    "def y_denormalizer(y_normalized):\n",
    "     y_original=y_normalized+1\n",
    "     return y_original\n",
    "x_init = x_normalizer(df_rese2.iloc[:,1:6].values)\n",
    "y_init = y_normalizer(np.transpose([df_rese2.iloc[:,-1].values]))\n",
    "X, Y = [x_init, y_init]\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "\n",
    "input_dim = len(X[0])\n",
    "print(input_dim)\n",
    "ker = GPy.kern.Matern32(input_dim = input_dim, ARD =True)#\n",
    "ker.lengthscale.constrain_bounded(1e-1, 10)\n",
    "ker.variance.constrain_bounded(1, 1000.0)\n",
    "\n",
    "#ker += GPy.kern.Bias(input_dim = input_dim)\n",
    "model_gpy = GPRegression(X , -Y, ker)#Emukit is a minimization tool; need to make Y negative\n",
    "model_gpy.Gaussian_noise.variance = 0.05**2\n",
    "model_gpy.Gaussian_noise.variance.fix()\n",
    "model_gpy.randomize()\n",
    "model_gpy.optimize_restarts(num_restarts=20,verbose =False, messages=False)\n",
    "objective_model = GPyModelWrapper(model_gpy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check the performance of the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_obj = objective_model.model.predict\n",
    "\n",
    "y_pred, y_uncer = f_obj(X)\n",
    "y_pred = -y_pred[:, -1]\n",
    "y_uncer = np.sqrt(y_uncer[:, -1])\n",
    "y_pred_unscaled = y_denormalizer(y_pred)\n",
    "\n",
    "print(y_pred_unscaled)\n",
    "print(y_uncer)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.5))  \n",
    "fs = 18\n",
    "lims1 = (1, 2)\n",
    "\n",
    "\n",
    "ax.scatter(y_denormalizer(Y[:, -1]), y_pred_unscaled, alpha=0.5, \n",
    "           c=(112/255, 161/255, 255/255), edgecolor='navy')\n",
    "ax.errorbar(y_denormalizer(Y[:, -1]), y_pred_unscaled, yerr=y_uncer, \n",
    "            ms=0, ls='', capsize=2, alpha=0.6, color='gray', zorder=0)\n",
    "ax.plot(lims1, lims1, 'k--', alpha=0.75, zorder=0)\n",
    "\n",
    "rmse_value = np.sqrt(mean_squared_error(Y[:, -1], y_pred))\n",
    "\n",
    "ax.set_xlabel('True Values', fontsize=fs)\n",
    "ax.set_ylabel('Predicted Values', fontsize=fs)\n",
    "ax.tick_params(labelsize=fs-2)\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new parameters for which predictions are needed\n",
    "new_params = np.array([[620, 250, 0.075, 0.02, 1]])\n",
    "\n",
    "# Use the model to predict new parameters, obtaining both the prediction values and uncertainty estimates\n",
    "new_pred, new_uncer = objective_model.model.predict(x_normalizer(new_params))\n",
    "\n",
    "new_pred = -new_pred[:, -1]\n",
    "new_pred_unscaled = y_denormalizer(new_pred)\n",
    "\n",
    "new_uncer = np.sqrt(new_uncer[:, -1])\n",
    "\n",
    "print(\"Prediction (after denormalization):\", new_pred_unscaled)\n",
    "print(\"Uncertainty Estimate:\", new_uncer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "mse = mean_squared_error\n",
    "mse_all = mse(Y[:,-1], y_pred)\n",
    "print ('all RMSE: %.4f' % (np.sqrt(mse_all)))\n",
    "\n",
    "rsquared_all = r2_score(Y[:,-1], y_pred)\n",
    "print ('all R^2: %.4f' % (rsquared_all))\n",
    "\n",
    "sprman_all = spearmanr(Y[:,-1], y_pred)\n",
    "print ('all spearman: %.4f' % (sprman_all[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the First Run of the Batch-mode Bayesian Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.bayesian_optimization.acquisitions import ExpectedImprovement, \\\n",
    "                                                      NegativeLowerConfidenceBound, \\\n",
    "                                                      MaxValueEntropySearch, \\\n",
    "                                                      MultipointExpectedImprovement,\\\n",
    "                                                      ProbabilityOfFeasibility, \\\n",
    "                                                      ProbabilityOfImprovement\n",
    "# Expeceted Improvement (EI)\n",
    "# acquisition = ExpectedImprovement(objective_model, jitter=0.2)\n",
    "## Lower Confidence Bound (LCB)\n",
    "# acquisition = NegativeLowerConfidenceBound(objective_model, beta = 10)\n",
    "## Probability Improvement (PI)\n",
    "# acquisition = ProbabilityOfImprovement(objective_model,jitter=0)\n",
    "\n",
    "\n",
    "# MaxValueEntropySearch (MES)\n",
    "acquisition_generator = lambda m: MaxValueEntropySearch(m, parameter_space)\n",
    "acquisition_integrated = IntegratedHyperParameterAcquisition(objective_model, acquisition_generator)\n",
    "acquisition = acquisition_integrated\n",
    "\n",
    "# Create a Bayesian optimization loop and collect new sample points\n",
    "bayesopt_cons_pr = ProbabilisticBayesianOptimizationLoop(\n",
    "    model_objective=objective_model,\n",
    "    space=parameter_space,\n",
    "    acquisition=acquisition,\n",
    "    batch_size=15  # batchsize>10 to account for duplication\n",
    ")\n",
    "\n",
    "X_new = bayesopt_cons_pr.candidate_point_calculator.compute_next_points(bayesopt_cons_pr.loop_state)\n",
    "X_new1= x_denormalizer(X_new)\n",
    "\n",
    "f_obj = objective_model.model.predict\n",
    "\n",
    "y_pred_new, y_uncer_new = f_obj(X_new)\n",
    "y_pred_new = -y_pred_new\n",
    "y_uncer_new = np.sqrt(y_uncer_new)\n",
    "print(y_pred_new)\n",
    "\n",
    "\n",
    "df_Xnew = pd.DataFrame(get_closest_array(X_new1), columns=df_rese2.columns[1:6])\n",
    "df_all = pd.concat([df_rese2.iloc[:, 1:6], df_Xnew])\n",
    "df_all_ = df_all.drop_duplicates()\n",
    "df_Xnew = df_Xnew.sort_values(by=list(df_rese2.columns[1:6]), ignore_index = True)\n",
    "df_Xnew.index = np.arange(len(df_Xnew))+len(df_rese2)\n",
    "df_Xnew.iloc[:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Xnew_array = df_Xnew.iloc[:, :].values\n",
    "result_array = get_closest_array(df_Xnew_array)\n",
    "\n",
    "print(result_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Surragate Model\n",
    "y_pred_new, y_uncer_new = f_obj(x_normalizer(result_array))\n",
    "y_pred_new = -y_pred_new\n",
    "y_uncer_new = np.sqrt(y_uncer_new)\n",
    "# Aquisition function\n",
    "f_acq = bayesopt_cons_pr.candidate_point_calculator.acquisition.evaluate \n",
    "acq_produc = f_acq(x_normalizer(result_array)) \n",
    "print(y_denormalizer(y_pred_new))\n",
    "print(acq_produc)\n",
    "\n",
    "\n",
    "X_new_columns = [f'X_new_{i}' for i in range(df_Xnew.shape[1])]\n",
    "X_new_data = {col: result_array[:, i] for i, col in enumerate(X_new_columns)}\n",
    "df_X_new = pd.DataFrame(X_new_data)\n",
    "df_X_new['y_pred_new'] =y_denormalizer(y_pred_new) \n",
    "df_X_new['y_uncer_new'] = y_uncer_new\n",
    "df_X_new['acq_produc'] = acq_produc\n",
    "\n",
    "\n",
    "output_filename = \"Next_round_suggestion.xlsx\"\n",
    "df_X_new.to_excel(output_filename, index=False)\n",
    "print(f\"The data has been successfully saved to an Excel file {output_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation of Optimization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emukit.core.initial_designs.random_design import RandomDesign\n",
    "np.random.seed=30\n",
    "design = RandomDesign(parameter_space)\n",
    "x_sampled = design.get_samples(200)\n",
    "x_columns = df_rese2.iloc[:,1:6].columns\n",
    "x_sampled_df = pd.DataFrame(x_denormalizer(x_sampled), columns=x_columns)\n",
    "\n",
    "\n",
    "for i in range(input_dim):\n",
    "    for j in range(input_dim-i-1):\n",
    "        \n",
    "## Generate a 2D grid for Contour plot\n",
    "        ind1 = i\n",
    "        ind2 = j+i+1\n",
    "        n_steps =21\n",
    "        x1x2y_pred, x1x2y_uncer =[[],[]]\n",
    "        for x1 in np.linspace(0, 1, n_steps):\n",
    "            for x2 in np.linspace(0, 1, n_steps):\n",
    "                x_temp = np.copy(x_sampled)\n",
    "                x_temp[:,ind1] = x1\n",
    "                x_temp[:,ind2] = x2\n",
    "                y_pred, y_uncer = f_obj(x_temp)\n",
    "                y_pred = -y_pred+1\n",
    "\n",
    "                x1_org = x_denormalizer(x_temp)[0,ind1]\n",
    "                x2_org = x_denormalizer(x_temp)[0,ind2]\n",
    "                x1x2y_pred.append([x1_org, x2_org, np.max(y_pred), np.mean(y_pred), np.min(y_pred)])\n",
    "                x1x2y_uncer.append([x1_org, x2_org, np.max(np.sqrt(y_uncer)), np.mean(np.sqrt(y_uncer)), np.min(np.sqrt(y_uncer))])\n",
    "        \n",
    "        x1 = np.array(x1x2y_pred, dtype=object)[:,0].reshape(n_steps, n_steps)\n",
    "        x2 = np.array(x1x2y_pred, dtype=object)[:,1].reshape(n_steps, n_steps)\n",
    "            \n",
    "        y_pred_max = np.array(x1x2y_pred, dtype=object)[:,2].reshape(n_steps, n_steps).astype(float)\n",
    "        y_pred_mean = np.array(x1x2y_pred, dtype=object)[:,3].reshape(n_steps, n_steps).astype(float)\n",
    "        y_pred_min = np.array(x1x2y_pred, dtype=object)[:,4].reshape(n_steps, n_steps).astype(float)\n",
    "        \n",
    "        y_uncer_max = np.array(x1x2y_uncer, dtype=object)[:,2].reshape(n_steps, n_steps).astype(float)\n",
    "        y_uncer_mean = np.array(x1x2y_uncer, dtype=object)[:,3].reshape(n_steps, n_steps).astype(float)\n",
    "        y_uncer_min = np.array(x1x2y_uncer, dtype=object)[:,4].reshape(n_steps, n_steps).astype(float)\n",
    "\n",
    "        fs = 20\n",
    "        title_pad = 16\n",
    "        \n",
    "## Contour for Prediction fractal dimension\n",
    "        fig,axes = plt.subplots(1, 3, figsize=(17, 3.5), sharey = False, sharex = False)\n",
    "        colorbar_offset = [12.5, 7, 4]\n",
    "        for ax, c_offset, y in zip(axes, colorbar_offset,\n",
    "                                   [y_pred_max, y_pred_mean, y_pred_min]):\n",
    "            \n",
    "            c_plt1 = ax.contourf(x1, x2, y, levels = np.arange(20)*0.05+1, cmap='plasma', extend = 'both')\n",
    "            cbar = fig.colorbar(c_plt1, ax= ax)\n",
    "            cbar.ax.tick_params(labelsize=fs*0.8)\n",
    "            ax.scatter(x_denormalizer(X)[:, ind1], \n",
    "                       x_denormalizer(X)[:, ind2], \n",
    "                       s = 50, facecolors='gray', alpha = 0.5, edgecolor = 'gray')\n",
    "            ax.scatter(x_denormalizer(X_new)[:, ind1], \n",
    "                      x_denormalizer(X_new)[:, ind2], \n",
    "                       s = 80, facecolors='green', alpha = 0.9, edgecolor = 'green')\n",
    "            \n",
    "            ax.set_xlabel(str(x_columns[ind1]),fontsize =  fs)\n",
    "            ax.set_ylabel(str(x_columns[ind2]),fontsize =  fs)\n",
    "\n",
    "            x1_delta = (np.max(x1)-np.min(x1))*0.02\n",
    "            x2_delta = (np.max(x2)-np.min(x2))*0.02\n",
    "            ax.set_xlim(np.min(x1)-x1_delta, np.max(x1)+x1_delta)\n",
    "            ax.set_ylim(np.min(x2)-x2_delta, np.max(x2)+x2_delta)\n",
    "            ax.tick_params(direction='in', length=5, width=1, labelsize = fs*.8)#, grid_alpha = 0.5\n",
    "            if ind1==0:#T_Re\n",
    "                ax.set_xticks([580, 615, 645, 680])\n",
    "            if ind1==1:#T_Se\n",
    "                ax.set_xticks([220, 240, 260, 280])\n",
    "            if ind1==2:#c_Re\n",
    "                ax.set_xticks([0.05, 0.1, 0.15])\n",
    "            if ind2==4:#sub.\n",
    "                ax.set_yticks([0, 1])\n",
    "            #ax.grid(True, linestyle='-.')\n",
    "\n",
    "        axes[0].set_title('objective fcn max', pad = title_pad,fontsize =  fs)\n",
    "        axes[1].set_title('objective fcn mean', pad = title_pad,fontsize =  fs)\n",
    "        axes[2].set_title('objective fcn min', pad = title_pad,fontsize =  fs)\n",
    "\n",
    "        plt.subplots_adjust(wspace = 0.3)\n",
    "        plt.show()\n",
    "# # Contour for Uncertainty        \n",
    "        fig,axes = plt.subplots(1, 3, figsize=(17, 3.5), sharey = False, sharex = False)\n",
    "        colorbar_offset = [3, 2.5, 2]\n",
    "        for ax, c_offset, y in zip(axes, colorbar_offset,\n",
    "                                   [y_uncer_max, y_uncer_mean, y_uncer_min]):\n",
    "\n",
    "            c_plt1 = ax.contourf(x1, x2, y,  levels = 0+np.arange(20)*0.01, cmap='plasma', extend = 'both')\n",
    "            cbar = fig.colorbar(c_plt1, ax= ax)\n",
    "            cbar.ax.tick_params(labelsize=fs*0.8)\n",
    "            ax.scatter(x_denormalizer(X)[:, ind1], \n",
    "                       x_denormalizer(X)[:, ind2], \n",
    "                       s = 50, facecolors='gray', alpha = 0.9, edgecolor = 'gray')\n",
    "            ax.scatter(x_denormalizer(X_new)[:, ind1], \n",
    "                       x_denormalizer(X_new)[:, ind2], \n",
    "                       s = 80, facecolors='green', alpha = 0.9, edgecolor = 'green')\n",
    "            ax.set_xlabel(str(x_columns[ind1]),fontsize =  fs)\n",
    "            ax.set_ylabel(str(x_columns[ind2]),fontsize =  fs)\n",
    "\n",
    "            x1_delta = (np.max(x1)-np.min(x1))*0.02\n",
    "            x2_delta = (np.max(x2)-np.min(x2))*0.02\n",
    "            ax.set_xlim(np.min(x1)-x1_delta, np.max(x1)+x1_delta)\n",
    "            ax.set_ylim(np.min(x2)-x2_delta, np.max(x2)+x2_delta)\n",
    "            ax.tick_params(direction='in', length=5, width=1, labelsize = fs*.8)#, grid_alpha = 0.5\n",
    "            if ind1==0:#T_Re\n",
    "                ax.set_xticks([580, 615, 645, 680])\n",
    "            if ind1==1:#T_Se\n",
    "                ax.set_xticks([220, 240, 260, 280])\n",
    "            if ind1==2:#c_Re\n",
    "                ax.set_xticks([0.05, 0.1, 0.15])\n",
    "            if ind2==4:#sub.\n",
    "                ax.set_yticks([0, 1])\n",
    "\n",
    "\n",
    "        axes[0].set_title('objective uncer max', pad = title_pad,fontsize =  fs)\n",
    "        axes[1].set_title('objective uncer mean', pad = title_pad,fontsize =  fs)\n",
    "        axes[2].set_title('objective uncer min', pad = title_pad,fontsize =  fs)\n",
    "        plt.subplots_adjust(wspace = 0.3)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
